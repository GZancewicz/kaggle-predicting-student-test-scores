# Experiment 14: Formula Discovery via Residual Analysis

Goal: Determine whether exam_score is generated by a relatively simple equation/rule set.

## Approach

1. Fit baseline Ridge (numerics + one-hot categoricals)
2. Analyze residual patterns (correlations, binned means, category effects)
3. Test formula-shaped transforms (poly, log, interactions, hinges)
4. Fit interpretable near-formula model with combined features

## Results

| Model | CV RMSE |
|-------|---------|
| Baseline Ridge | 8.8948 |
| + study*attendance | 8.8937 |
| + hinge(study-6) | 8.8936 |
| Combined transforms | 8.8925 |
| **Formula model (42 features)** | **8.8885** |

### Key Findings

**Residual correlations are essentially zero:**
- age: -0.0000
- study_hours: +0.0000
- class_attendance: -0.0000
- sleep_hours: -0.0000

**Top residual patterns (all tiny):**
1. study*attendance: -0.0034
2. study_hours^2: -0.0026
3. study/sleep: +0.0014

**Categorical effects also negligible** (max ~0.0002)

### Comparison

| Model | CV RMSE | Gap |
|-------|---------|-----|
| Best Ensemble (Exp 12) | 8.7563 | - |
| **Formula model** | 8.8885 | +0.1322 |

## Conclusion

**Formula model is 0.13 RMSE worse than tree ensemble.**

The data generator is NOT a simple formula. Residuals from Ridge are essentially uncorrelated with all features - the linear model is already capturing all the linear signal. The remaining 0.13+ RMSE gap requires nonlinear tree-based modeling to capture.

Tree ensembles (LightGBM, CatBoost) are finding real nonlinear structure that can't be expressed as simple polynomial/interaction/hinge formulas.

## Usage

```bash
python3 train.py
```
