"""
Kaggle Playground Series S6E1: Predicting Student Test Scores
Experiment 14: Formula Discovery via Residual Analysis

Goal: Determine whether exam_score is generated by a relatively simple equation/rule set.

Approach:
1. Fit baseline Ridge, compute residuals
2. Analyze residual patterns (correlations, binned means, category effects)
3. Try formula-shaped transforms (poly, log, interactions, hinges)
4. Fit interpretable near-formula model
5. Compare to best ensemble (8.7563)
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
from sklearn.linear_model import Ridge, ElasticNet
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.metrics import mean_squared_error
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

# Configuration
N_FOLDS = 5
SEED = 42
CLIP_MIN, CLIP_MAX = 19, 100
BEST_ENSEMBLE = 8.7563  # Exp 12 result


def load_data():
    train = pd.read_csv('../../data/train.csv')
    test = pd.read_csv('../../data/test.csv')
    return train, test


def get_fold_indices(n_samples, n_folds=5, random_state=42):
    kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)
    return list(kf.split(np.arange(n_samples)))


def prepare_baseline_features(train, test):
    """Prepare features for baseline Ridge: numerics + one-hot categoricals."""
    num_cols = ['age', 'study_hours', 'class_attendance', 'sleep_hours']
    cat_cols = ['gender', 'course', 'internet_access', 'sleep_quality',
                'study_method', 'facility_rating', 'exam_difficulty']

    # One-hot encode categoricals
    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
    train_cat = encoder.fit_transform(train[cat_cols])
    test_cat = encoder.transform(test[cat_cols])

    # Combine
    X_train = np.hstack([train[num_cols].values, train_cat])
    X_test = np.hstack([test[num_cols].values, test_cat])

    return X_train, X_test, num_cols, cat_cols


def train_baseline_ridge(X_train, y_train, fold_indices):
    """Train Ridge and get OOF predictions."""
    print("\n" + "="*70)
    print("STEP 1: BASELINE RIDGE MODEL")
    print("="*70)

    scaler = StandardScaler()
    oof_preds = np.zeros(len(X_train))
    fold_rmses = []

    for fold, (train_idx, val_idx) in enumerate(fold_indices):
        X_tr, X_val = X_train[train_idx], X_train[val_idx]
        y_tr, y_val = y_train[train_idx], y_train[val_idx]

        # Scale
        X_tr_scaled = scaler.fit_transform(X_tr)
        X_val_scaled = scaler.transform(X_val)

        # Fit Ridge
        model = Ridge(alpha=1.0)
        model.fit(X_tr_scaled, y_tr)

        oof_preds[val_idx] = model.predict(X_val_scaled)
        fold_rmse = np.sqrt(mean_squared_error(y_val, oof_preds[val_idx]))
        fold_rmses.append(fold_rmse)
        print(f"  Fold {fold+1}: {fold_rmse:.4f}")

    cv_rmse = np.sqrt(mean_squared_error(y_train, oof_preds))
    print(f"\n>>> Baseline Ridge CV: {cv_rmse:.4f}")

    return oof_preds, cv_rmse


def analyze_residuals(train, y_train, oof_preds):
    """Analyze residual patterns to discover formula structure."""
    print("\n" + "="*70)
    print("STEP 2: RESIDUAL ANALYSIS")
    print("="*70)

    residuals = y_train - oof_preds

    num_cols = ['age', 'study_hours', 'class_attendance', 'sleep_hours']
    cat_cols = ['gender', 'course', 'internet_access', 'sleep_quality',
                'study_method', 'facility_rating', 'exam_difficulty']

    patterns = []

    # Numeric features: correlation and binned means
    print("\n--- Numeric Feature Residual Correlations ---")
    for col in num_cols:
        corr = np.corrcoef(train[col].values, residuals)[0, 1]
        patterns.append((f"{col}_corr", abs(corr), corr, 'numeric'))
        print(f"  {col}: {corr:+.4f}")

    # Binned residual means for key features
    print("\n--- Binned Residual Means (study_hours) ---")
    bins = pd.qcut(train['study_hours'], q=10, duplicates='drop')
    binned_means = pd.DataFrame({'bin': bins, 'residual': residuals}).groupby('bin')['residual'].mean()
    print(binned_means.to_string())

    # Check for nonlinearity: residual vs study_hours^2
    study_sq = train['study_hours'] ** 2
    corr_sq = np.corrcoef(study_sq.values, residuals)[0, 1]
    print(f"\n  study_hours^2 residual corr: {corr_sq:+.4f}")
    patterns.append(("study_hours^2_corr", abs(corr_sq), corr_sq, 'transform'))

    # Categorical features: mean residual per category
    print("\n--- Categorical Feature Residual Effects ---")
    for col in cat_cols:
        cat_means = pd.DataFrame({'cat': train[col], 'residual': residuals}).groupby('cat')['residual'].mean()
        max_effect = cat_means.abs().max()
        patterns.append((f"{col}_cat_effect", max_effect, cat_means.to_dict(), 'categorical'))
        print(f"  {col}:")
        for cat, mean_r in cat_means.items():
            print(f"    {cat}: {mean_r:+.4f}")

    # Interaction residual correlations
    print("\n--- Interaction Residual Correlations ---")
    interactions = [
        ('study_x_attendance', train['study_hours'] * train['class_attendance']),
        ('study_per_sleep', train['study_hours'] / (train['sleep_hours'] + 1)),
        ('attendance_per_sleep', train['class_attendance'] / (train['sleep_hours'] + 1)),
    ]
    for name, values in interactions:
        corr = np.corrcoef(values, residuals)[0, 1]
        patterns.append((name, abs(corr), corr, 'interaction'))
        print(f"  {name}: {corr:+.4f}")

    # Sort by effect size
    patterns.sort(key=lambda x: x[1], reverse=True)

    print("\n--- TOP 10 RESIDUAL PATTERNS ---")
    for i, (name, effect, detail, ptype) in enumerate(patterns[:10]):
        if ptype == 'categorical':
            print(f"  {i+1}. {name}: max_effect={effect:.4f}")
        else:
            print(f"  {i+1}. {name}: {detail:+.4f}")

    return residuals, patterns


def test_formula_transforms(train, test, y_train, fold_indices):
    """Test formula-shaped transforms to reduce residual structure."""
    print("\n" + "="*70)
    print("STEP 3: TESTING FORMULA-SHAPED TRANSFORMS")
    print("="*70)

    num_cols = ['age', 'study_hours', 'class_attendance', 'sleep_hours']
    cat_cols = ['gender', 'course', 'internet_access', 'sleep_quality',
                'study_method', 'facility_rating', 'exam_difficulty']

    # Base features
    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
    train_cat = encoder.fit_transform(train[cat_cols])
    test_cat = encoder.transform(test[cat_cols])

    X_base_train = np.hstack([train[num_cols].values, train_cat])
    X_base_test = np.hstack([test[num_cols].values, test_cat])

    results = []

    # Test individual transforms
    transforms = {
        'study_hours^2': (train['study_hours'] ** 2, test['study_hours'] ** 2),
        'sqrt(study_hours)': (np.sqrt(train['study_hours']), np.sqrt(test['study_hours'])),
        'log1p(study_hours)': (np.log1p(train['study_hours']), np.log1p(test['study_hours'])),
        'attendance^2': (train['class_attendance'] ** 2, test['class_attendance'] ** 2),
        'study*attendance': (train['study_hours'] * train['class_attendance'],
                            test['study_hours'] * test['class_attendance']),
        'study/sleep': (train['study_hours'] / (train['sleep_hours'] + 1),
                       test['study_hours'] / (test['sleep_hours'] + 1)),
        'attendance/sleep': (train['class_attendance'] / (train['sleep_hours'] + 1),
                            test['class_attendance'] / (test['sleep_hours'] + 1)),
        'hinge(study-4)': (np.maximum(0, train['study_hours'] - 4),
                          np.maximum(0, test['study_hours'] - 4)),
        'hinge(study-6)': (np.maximum(0, train['study_hours'] - 6),
                          np.maximum(0, test['study_hours'] - 6)),
        'hinge(attend-50)': (np.maximum(0, train['class_attendance'] - 50),
                            np.maximum(0, test['class_attendance'] - 50)),
    }

    # Test baseline first
    baseline_rmse = cv_ridge(X_base_train, y_train, fold_indices)
    results.append(('baseline', baseline_rmse))
    print(f"\n  baseline: {baseline_rmse:.4f}")

    # Test each transform
    for name, (tr_feat, te_feat) in transforms.items():
        X_tr = np.hstack([X_base_train, tr_feat.values.reshape(-1, 1)])
        rmse = cv_ridge(X_tr, y_train, fold_indices)
        results.append((name, rmse))
        delta = baseline_rmse - rmse
        print(f"  + {name}: {rmse:.4f} ({delta:+.4f})")

    # Try combining best transforms
    print("\n--- Combining Best Transforms ---")
    best_transforms = [name for name, rmse in results[1:] if baseline_rmse - rmse > 0.001]
    if best_transforms:
        print(f"  Best individual: {best_transforms}")

        # Add all beneficial transforms
        X_combined_train = X_base_train.copy()
        X_combined_test = X_base_test.copy()
        for name in best_transforms:
            tr_feat, te_feat = transforms[name]
            X_combined_train = np.hstack([X_combined_train, tr_feat.values.reshape(-1, 1)])
            X_combined_test = np.hstack([X_combined_test, te_feat.values.reshape(-1, 1)])

        combined_rmse = cv_ridge(X_combined_train, y_train, fold_indices)
        results.append(('combined_best', combined_rmse))
        print(f"  Combined: {combined_rmse:.4f} ({baseline_rmse - combined_rmse:+.4f} vs baseline)")
    else:
        print("  No transforms improved by >0.001")
        X_combined_train, X_combined_test = X_base_train, X_base_test
        combined_rmse = baseline_rmse

    return results, X_combined_train, X_combined_test


def cv_ridge(X_train, y_train, fold_indices, alpha=1.0):
    """Quick CV evaluation with Ridge."""
    scaler = StandardScaler()
    oof_preds = np.zeros(len(X_train))

    for train_idx, val_idx in fold_indices:
        X_tr, X_val = X_train[train_idx], X_train[val_idx]
        y_tr = y_train[train_idx]

        X_tr_scaled = scaler.fit_transform(X_tr)
        X_val_scaled = scaler.transform(X_val)

        model = Ridge(alpha=alpha)
        model.fit(X_tr_scaled, y_tr)
        oof_preds[val_idx] = model.predict(X_val_scaled)

    return np.sqrt(mean_squared_error(y_train, oof_preds))


def fit_interpretable_model(train, test, y_train, fold_indices):
    """Fit an interpretable near-formula model with splines/hinges."""
    print("\n" + "="*70)
    print("STEP 4: INTERPRETABLE NEAR-FORMULA MODEL")
    print("="*70)

    # Build feature set with polynomial and piecewise terms
    features_train = []
    features_test = []
    feature_names = []

    # Raw numerics
    for col in ['age', 'study_hours', 'class_attendance', 'sleep_hours']:
        features_train.append(train[col].values)
        features_test.append(test[col].values)
        feature_names.append(col)

    # Polynomial terms for study_hours and attendance
    for col in ['study_hours', 'class_attendance']:
        features_train.append(train[col].values ** 2)
        features_test.append(test[col].values ** 2)
        feature_names.append(f'{col}^2')

    # Log transforms
    features_train.append(np.log1p(train['study_hours'].values))
    features_test.append(np.log1p(test['study_hours'].values))
    feature_names.append('log1p(study_hours)')

    # Interactions
    features_train.append(train['study_hours'].values * train['class_attendance'].values)
    features_test.append(test['study_hours'].values * test['class_attendance'].values)
    feature_names.append('study*attendance')

    features_train.append(train['study_hours'].values / (train['sleep_hours'].values + 1))
    features_test.append(test['study_hours'].values / (test['sleep_hours'].values + 1))
    feature_names.append('study/sleep')

    # Piecewise/hinge functions
    for threshold in [2, 4, 6, 8]:
        features_train.append(np.maximum(0, train['study_hours'].values - threshold))
        features_test.append(np.maximum(0, test['study_hours'].values - threshold))
        feature_names.append(f'hinge(study-{threshold})')

    for threshold in [30, 50, 70]:
        features_train.append(np.maximum(0, train['class_attendance'].values - threshold))
        features_test.append(np.maximum(0, test['class_attendance'].values - threshold))
        feature_names.append(f'hinge(attend-{threshold})')

    # Categorical encoding (target encoding style)
    cat_cols = ['gender', 'course', 'internet_access', 'sleep_quality',
                'study_method', 'facility_rating', 'exam_difficulty']

    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
    train_cat = encoder.fit_transform(train[cat_cols])
    test_cat = encoder.transform(test[cat_cols])

    X_train = np.column_stack(features_train + [train_cat])
    X_test = np.column_stack(features_test + [test_cat])

    print(f"  Total features: {X_train.shape[1]}")
    print(f"  Numeric + transforms: {len(feature_names)}")
    print(f"  Categorical one-hot: {train_cat.shape[1]}")

    # Try different alpha values
    best_rmse = float('inf')
    best_alpha = 1.0

    print("\n--- Tuning Ridge alpha ---")
    for alpha in [0.01, 0.1, 1.0, 10.0, 100.0]:
        rmse = cv_ridge(X_train, y_train, fold_indices, alpha=alpha)
        print(f"  alpha={alpha}: {rmse:.4f}")
        if rmse < best_rmse:
            best_rmse = rmse
            best_alpha = alpha

    print(f"\n  Best alpha: {best_alpha}")
    print(f"  Best CV RMSE: {best_rmse:.4f}")

    # Also try ElasticNet
    print("\n--- Trying ElasticNet ---")
    scaler = StandardScaler()
    oof_preds_en = np.zeros(len(X_train))

    for train_idx, val_idx in fold_indices:
        X_tr, X_val = X_train[train_idx], X_train[val_idx]
        y_tr = y_train[train_idx]

        X_tr_scaled = scaler.fit_transform(X_tr)
        X_val_scaled = scaler.transform(X_val)

        model = ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=5000)
        model.fit(X_tr_scaled, y_tr)
        oof_preds_en[val_idx] = model.predict(X_val_scaled)

    en_rmse = np.sqrt(mean_squared_error(y_train, oof_preds_en))
    print(f"  ElasticNet CV: {en_rmse:.4f}")

    # Use best model
    if en_rmse < best_rmse:
        best_rmse = en_rmse
        print("  Using ElasticNet")
    else:
        print("  Using Ridge")

    return best_rmse, X_train, X_test, best_alpha


def create_submission(test_ids, predictions, filename):
    pd.DataFrame({'id': test_ids, 'exam_score': predictions}).to_csv(filename, index=False)
    print(f"Saved {filename}")


def main():
    print("="*70)
    print("EXPERIMENT 14: FORMULA DISCOVERY VIA RESIDUAL ANALYSIS")
    print("="*70)

    print("\nLoading data...")
    train, test = load_data()
    print(f"Train: {train.shape}, Test: {test.shape}")

    y_train = train['exam_score'].values
    test_ids = test['id'].values

    fold_indices = get_fold_indices(len(train), N_FOLDS, random_state=SEED)

    # Step 1: Baseline Ridge
    X_base_train, X_base_test, num_cols, cat_cols = prepare_baseline_features(train, test)
    oof_preds, baseline_rmse = train_baseline_ridge(X_base_train, y_train, fold_indices)

    # Step 2: Residual Analysis
    residuals, patterns = analyze_residuals(train, y_train, oof_preds)

    # Step 3: Test formula transforms
    transform_results, X_transform_train, X_transform_test = test_formula_transforms(
        train, test, y_train, fold_indices
    )

    # Step 4: Interpretable model
    formula_rmse, X_formula_train, X_formula_test, best_alpha = fit_interpretable_model(
        train, test, y_train, fold_indices
    )

    # Final Summary
    print("\n" + "="*70)
    print("FINAL SUMMARY")
    print("="*70)

    print(f"\n  Baseline Ridge:       {baseline_rmse:.4f}")
    print(f"  Best transform combo: {transform_results[-1][1]:.4f}")
    print(f"  Formula model:        {formula_rmse:.4f}")
    print(f"\n  Best ensemble (Exp12): {BEST_ENSEMBLE:.4f}")
    print(f"  Gap to beat:           {formula_rmse - BEST_ENSEMBLE:+.4f}")

    # Decision
    improvement = BEST_ENSEMBLE - formula_rmse
    print(f"\n>>> DECISION:")
    if improvement >= 0.01:
        print(f"    Formula model beats ensemble by {improvement:.4f} - generating submission!")

        # Train final model and predict
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_formula_train)
        X_test_scaled = scaler.transform(X_formula_test)

        model = Ridge(alpha=best_alpha)
        model.fit(X_train_scaled, y_train)
        test_preds = model.predict(X_test_scaled)
        test_preds = np.clip(test_preds, CLIP_MIN, CLIP_MAX)

        create_submission(test_ids, test_preds, 'submission_exp14_formula.csv')
    else:
        print(f"    Formula model does NOT beat ensemble (gap: {improvement:+.4f})")
        print(f"    Conclusion: Generator is either complex or too noisy for simple formulas")

    # Save analysis artifacts
    np.save('residuals.npy', residuals)
    np.save('oof_baseline.npy', oof_preds)

    print("\nDone!")


if __name__ == '__main__':
    main()
